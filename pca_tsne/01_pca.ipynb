{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_folder):\n\u001b[1;32m     18\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_folder)\n\u001b[0;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# variable data\u001b[39;00m\n\u001b[1;32m     25\u001b[0m cath_ids \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m1\u001b[39m: {\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;241m10\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morthogonal_bundle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m20\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mup_down_bundle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m25\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_horseshoe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m40\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_solenoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m50\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_alpha_barrel\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     }\n\u001b[1;32m     46\u001b[0m }\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "#ALL ARCHITECTURES VISUAL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Load the dataset\n",
    "\n",
    "path = \"/Volumes/dax-hd/project-data/search-files/merged-data.csv\"\n",
    "save_folder = \"/Volumes/dax-hd/project-data/images/all_arch_pca\"\n",
    "\n",
    "plot_path = os.path.join('all_arch_pca')\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "selected_columns = [\n",
    "    \"hydrophobic_fitness\",\n",
    "    \"isoelectric_point\",\n",
    "    \"charge\",\n",
    "    \"mass\",\n",
    "    \"num_residues\",\n",
    "    \"packing_density\",\n",
    "    \"budeff_total\",\n",
    "    \"evoef2_ref_total\", \n",
    "    \"dfire2_total\",\n",
    "    \"rosetta_total\",\n",
    "    \"aggrescan3d_avg_value\"\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "df_selected = df[selected_columns]\n",
    "\n",
    "df_selected_cleaned = df_selected.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_selected_cleaned)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])\n",
    "\n",
    "pca_df.to_csv('pca_results.csv', index=False)\n",
    "\n",
    "pca_df['architecture_name'] = df['architecture_name'][df_selected_cleaned.index].reset_index(drop=True)\n",
    "\n",
    "print(\"Variance explained by each component:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.tight_layout()\n",
    "cumulative_variance_plot_path = os.path.join(save_folder, 'cumulative_explained_variance.png')\n",
    "plt.savefig(cumulative_variance_plot_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=pca_df['architecture_name'], palette='Spectral', legend='full')\n",
    "plt.title('PCA on Dataset by Protein Architecture')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Architecture', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "scatter_plot_path = os.path.join(save_folder, 'pca_scatter_plot_by_architecture.png')\n",
    "plt.savefig(scatter_plot_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "for i in range(num_components_to_visualize):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    component_loadings = pca.components_[i]\n",
    "    indices = np.argsort(abs(component_loadings))[::-1]\n",
    "    plt.bar(feature_names[indices], component_loadings[indices])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f'PCA Component {i+1} Loadings')\n",
    "    plt.ylabel('Loading Value')\n",
    "    feature_contribution_path = os.path.join(save_folder, f'pca_component_{i+1}_loadings.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(feature_contribution_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL TOPOLOGIES BY ARCHITECTURES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Load the dataset\n",
    "\n",
    "path = \"/Volumes/dax-hd/project-data/search-files/merged-data.csv\"\n",
    "base_save_folder = \"/Volumes/dax-hd/project-data/images/pca_topology_\"\n",
    "cath_dict_path = \"/Volumes/dax-hd/project-data/search-files/cath-archetype-dict.txt\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "with open(cath_dict_path, 'r') as file:\n",
    "    cath_dict = json.load(file)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Add the architecture name to df\n",
    "\n",
    "def add_topology_description(df, cath_dict):\n",
    "    def get_topology_description(row):\n",
    "        class_num = str(row['Class number'])\n",
    "        arch_num = str(row['Architecture number'])\n",
    "        top_num = str(row['Topology number'])\n",
    "        try:\n",
    "            description = cath_dict[class_num][arch_num][top_num]['description']\n",
    "            return description\n",
    "        except KeyError:\n",
    "            return \"Unknown\"\n",
    "    \n",
    "    df['topology_description'] = df.apply(get_topology_description, axis=1)\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "df = add_topology_description(df, cath_dict)\n",
    "\n",
    "if not os.path.exists(base_save_folder):\n",
    "    os.makedirs(base_save_folder)\n",
    "\n",
    "selected_architectures = [\"sandwich (2,60)\"]\n",
    "\n",
    "selected_columns = [\n",
    "    \n",
    "    # \"composition_ALA\",\"composition_CYS\",\"composition_ASP\",\"composition_GLU\",\"composition_PHE\",\"composition_GLY\",\n",
    "    # \"composition_HIS\",\"composition_ILE\",\"composition_LYS\",\"composition_LEU\",\"composition_MET\",\"composition_ASN\",\n",
    "    # \"composition_PRO\",\"composition_GLN\",\"composition_ARG\",\"composition_SER\",\"composition_THR\",\"composition_VAL\",\n",
    "    # \"composition_TRP\",\"composition_UNK\",\"composition_TYR\",\n",
    "    # \"ss_prop_alpha_helix\",\"ss_prop_beta_bridge\",\"ss_prop_beta_strand\",\"ss_prop_3_10_helix\",\"ss_prop_pi_helix\",\n",
    "    # \"ss_prop_hbonded_turn\",\"ss_prop_bend\",\"ss_prop_loop\",\n",
    "    \"hydrophobic_fitness\",\n",
    "    \"isoelectric_point\",\n",
    "    \"charge\",\n",
    "    \"mass\",\n",
    "    # \"num_residues\",\n",
    "    \"packing_density\",\n",
    "    # \"budeff_total\",\n",
    "    # \"evoef2_ref_total\",\n",
    "    # \"dfire2_total\",\n",
    "    \"rosetta_total\",\n",
    "    \"aggrescan3d_avg_value\"\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "def filter_for_archetypes(df, cath_dict):\n",
    "    archetype_ids = []\n",
    "    for _, row in df.iterrows():\n",
    "        class_num = str(row['Class number'])\n",
    "        arch_num = str(row['Architecture number'])\n",
    "        top_num = str(row['Topology number'])\n",
    "        try:\n",
    "            protein_id = cath_dict[class_num][arch_num][top_num]['protein_id']\n",
    "            if protein_id[:4] in row['design_name']:\n",
    "                archetype_ids.append(row['design_name'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return df[df['design_name'].isin(archetype_ids)]\n",
    "\n",
    "df_filtered = filter_for_archetypes(df, cath_dict)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "df_selected_cleaned = df_filtered[selected_columns].dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "for architecture_name in df['architecture_name'].unique():\n",
    "    if architecture_name not in selected_architectures:\n",
    "        continue\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_filtered = df[df['architecture_name'] == architecture_name]\n",
    "    df_selected = df_filtered[selected_columns]\n",
    "    df_selected_cleaned = df_selected.dropna()\n",
    "    \n",
    "    if df_selected_cleaned.empty:\n",
    "        continue\n",
    "\n",
    "    save_folder = os.path.join(base_save_folder, architecture_name.replace('/', '_'))\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    \n",
    "    # Standardize and PCA\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df_selected_cleaned)\n",
    "    robust_scaler = RobustScaler()\n",
    "    scaled_features_robust = robust_scaler.fit_transform(df_selected_cleaned)\n",
    "    scaled_features = scaler.fit_transform(df_selected_cleaned)\n",
    "    pca = PCA(n_components=0.95)\n",
    "    principal_components = pca.fit_transform(scaled_features)\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])\n",
    "    pca_df['topology_description'] = df_filtered['topology_description'][df_selected_cleaned.index].reset_index(drop=True)\n",
    "    \n",
    "    # pca_df.to_csv(\"/Volumes/dax-hd/project-data/csv.csv\", index=False)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------\n",
    "    # Plotting\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    unique_topologies = pca_df['topology_description'].unique()\n",
    "    palette = sns.color_palette('tab20c', n_colors=len(unique_topologies))\n",
    "    topology_color_mapping = {topology: color for topology, color in zip(unique_topologies, palette)}\n",
    "\n",
    "    scatter_plot = sns.scatterplot(\n",
    "        x='PC1', y='PC2',\n",
    "        hue='topology_description',\n",
    "        palette=topology_color_mapping, \n",
    "        data=pca_df, s=100 \n",
    "    )\n",
    "    plt.title(f'PCA for {architecture_name}')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "\n",
    "    # Handling legend\n",
    "    handles, labels = scatter_plot.get_legend_handles_labels()\n",
    "    plt.legend([],[], frameon=False)\n",
    "    plt.savefig(os.path.join(save_folder, 'pca_scatter_by_topology.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Save legend separately\n",
    "    fig_legend = plt.figure(figsize=(3, 4))\n",
    "    plt.figlegend(handles, labels, loc='center')\n",
    "    plt.savefig(os.path.join(save_folder, 'legend.png'), bbox_inches='tight')\n",
    "    plt.close(fig_legend)\n",
    "\n",
    "    # Plotting cumulative variance\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.tight_layout()\n",
    "    cumulative_variance_plot_path = os.path.join(save_folder, 'cumulative_explained_variance.png')\n",
    "    plt.savefig(cumulative_variance_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    #Component loading plot\n",
    "    for i in range(pca.n_components_):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        component_loadings = pca.components_[i]\n",
    "        indices = np.argsort(abs(component_loadings))[::-1]\n",
    "        feature_names = np.array(selected_columns)[indices]\n",
    "        plt.bar(range(len(indices)), component_loadings[indices])\n",
    "        plt.xticks(range(len(indices)), feature_names, rotation=90)\n",
    "        plt.title(f'PCA Component {i+1} Loadings')\n",
    "        plt.ylabel('Loading Value')\n",
    "        feature_contribution_path = os.path.join(save_folder, f'pca_component_{i+1}_loadings.png')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(feature_contribution_path)\n",
    "        plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each component:\n",
      "[0.34253849 0.22998887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/0p2hf3h96gz7n16ykmw22n9w0000gn/T/ipykernel_894/1003490062.py:91: UserWarning: KDE cannot be estimated (0 variance or perfect covariance). Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(\n",
      "/var/folders/fg/0p2hf3h96gz7n16ykmw22n9w0000gn/T/ipykernel_894/1003490062.py:91: UserWarning: KDE cannot be estimated (0 variance or perfect covariance). Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(\n"
     ]
    }
   ],
   "source": [
    "#ALL ARCHITECTURES WITH KDE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Load the dataset\n",
    "\n",
    "path = \"/Volumes/dax-hd/project-data/search-files/merged-data.csv\"\n",
    "base_save_folder = \"/Volumes/dax-hd/project-data/images/pca_architectures_all\"\n",
    "cath_dict_path = \"/Volumes/dax-hd/project-data/search-files/cath-archetype-dict.txt\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "with open(cath_dict_path, 'r') as file:\n",
    "    cath_dict = json.load(file)\n",
    "\n",
    "if not os.path.exists(base_save_folder):\n",
    "    os.makedirs(base_save_folder)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "selected_columns = [\n",
    "    \"composition_ALA\",\"composition_CYS\",\"composition_ASP\",\"composition_GLU\",\"composition_PHE\",\"composition_GLY\",\n",
    "    \"composition_HIS\",\"composition_ILE\",\"composition_LYS\",\"composition_LEU\",\"composition_MET\",\"composition_ASN\",\n",
    "    \"composition_PRO\",\"composition_GLN\",\"composition_ARG\",\"composition_SER\",\"composition_THR\",\"composition_VAL\",\n",
    "    \"composition_TRP\",\"composition_UNK\",\"composition_TYR\",\n",
    "    \"ss_prop_alpha_helix\",\"ss_prop_beta_bridge\",\"ss_prop_beta_strand\",\n",
    "    \"ss_prop_3_10_helix\",\"ss_prop_pi_helix\",\n",
    "    \"ss_prop_hbonded_turn\",\"ss_prop_bend\",\"ss_prop_loop\",\n",
    "    \"hydrophobic_fitness\",\n",
    "    \"isoelectric_point\",\n",
    "    \"charge\",\n",
    "    \"mass\",\n",
    "    \"num_residues\",\n",
    "    \"packing_density\",\n",
    "    \"budeff_total\",\n",
    "    \"evoef2_ref_total\",\n",
    "    \"dfire2_total\",\n",
    "    \"rosetta_total\",\n",
    "    \"aggrescan3d_avg_value\"\n",
    "]\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "df_selected = df[selected_columns]\n",
    "\n",
    "df_selected_cleaned = df_selected.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_selected_cleaned)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "pca_df['architecture_name'] = df['architecture_name'][df_selected.index].reset_index(drop=True)\n",
    "\n",
    "print(\"Variance explained by each component:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "# Plotting PCA scatter with KDE\n",
    "plt.figure(figsize=(30, 20))\n",
    "unique_architectures = pca_df['architecture_name'].unique()\n",
    "palette = sns.color_palette(\"tab20b\") + sns.color_palette(\"tab20c\")\n",
    "architecture_color_mapping = {architecture: color for architecture, color in zip(unique_architectures, palette)}\n",
    "\n",
    "# KDE overlay for each architecture\n",
    "for architecture, group_df in pca_df.groupby('architecture_name'):\n",
    "    sns.kdeplot(\n",
    "        x=group_df['PC1'], y=group_df['PC2'], \n",
    "        color=architecture_color_mapping[architecture],\n",
    "        levels=2, fill=True, alpha=0.4\n",
    "    )\n",
    "\n",
    "# Scatter plot for visualization\n",
    "sns.scatterplot(\n",
    "    x='PC1', y='PC2',\n",
    "    hue='architecture_name', palette=architecture_color_mapping,\n",
    "    data=pca_df, legend='full', s=100\n",
    ")\n",
    "plt.title('PCA on Dataset by Protein Architecture')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Architecture', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "scatter_plot_path = os.path.join(base_save_folder, 'pca_scatter_plot_by_architecture_with_kde.png')\n",
    "plt.savefig(scatter_plot_path, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each component: [0.61876376 0.11647977]\n",
      "Number of data points for architecture 'aligned_prism (2,100)': 1\n",
      "Number of data points for architecture 'alpha_alpha_barrel (1,50)': 6\n",
      "Number of data points for architecture 'alpha_beta_barrel (3,20)': 41\n",
      "Number of data points for architecture 'alpha_beta_complex (3,90)': 11\n",
      "Number of data points for architecture 'alpha_beta_horseshoe (3,80)': 4\n",
      "Number of data points for architecture 'alpha_beta_prism (3,65)': 2\n",
      "Number of data points for architecture 'alpha_beta_roll (3,10)': 92\n",
      "Number of data points for architecture 'alpha_horseshoe (1,25)': 19\n",
      "Number of data points for architecture 'alpha_solenoid (1,40)': 2\n",
      "Number of data points for architecture 'beta_barrel (2,40)': 118\n",
      "Number of data points for architecture 'beta_complex (2,170)': 32\n",
      "Number of data points for architecture 'beta_roll (2,30)': 68\n",
      "Number of data points for architecture 'box (3,70)': 1\n",
      "Number of data points for architecture 'clam (2,50)': 2\n",
      "Number of data points for architecture 'distorted_sandwich (2,70)': 20\n",
      "Number of data points for architecture 'eight_propeller (2,140)': 2\n",
      "Number of data points for architecture 'five_propeller (2,115)': 1\n",
      "Number of data points for architecture 'five_stranded_propeller (3,75)': 2\n",
      "Number of data points for architecture 'four_layer_sandwich (3,60)': 19\n",
      "Number of data points for architecture 'four_propeller (2,110)': 1\n",
      "Number of data points for architecture 'orthogonal_bundle (1,10)': 448\n",
      "Number of data points for architecture 'orthogonal_prism (2,90)': 2\n",
      "Number of data points for architecture 'ribbon (2,10)': 38\n",
      "Number of data points for architecture 'sandwich (2,60)': 116\n",
      "Number of data points for architecture 'seven_propeller (2,130)': 4\n",
      "Number of data points for architecture 'shell (2,180)': 1\n",
      "Number of data points for architecture 'single_sheet (2,20)': 39\n",
      "Number of data points for architecture 'six_propeller (2,120)': 4\n",
      "Number of data points for architecture 'super_roll (3,15)': 3\n",
      "Number of data points for architecture 'three_layer_aba_sandwich (3,40)': 310\n",
      "Number of data points for architecture 'three_layer_bab_sandwich (3,50)': 25\n",
      "Number of data points for architecture 'three_layer_bba_sandwich (3,55)': 9\n",
      "Number of data points for architecture 'three_layer_sandwich (2,102)': 3\n",
      "Number of data points for architecture 'three_propeller (2,105)': 1\n",
      "Number of data points for architecture 'three_solenoid (2,160)': 6\n",
      "Number of data points for architecture 'trefoil (2,80)': 5\n",
      "Number of data points for architecture 'two_layer_sandwich (3,30)': 427\n",
      "Number of data points for architecture 'two_solenoid (2,150)': 1\n",
      "Number of data points for architecture 'up_down_bundle (1,20)': 178\n"
     ]
    }
   ],
   "source": [
    "#ALL ARCHITECTURES WITH JUST ARCHETYPAL TOPOLOGIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Load the dataset\n",
    "\n",
    "path = \"/Volumes/dax-hd/project-data/search-files/merged-data.csv\"\n",
    "base_save_folder = \"/Volumes/dax-hd/project-data/images/pca_architectures_topology_archetypes\"\n",
    "cath_dict_path = \"/Volumes/dax-hd/project-data/search-files/cath-archetype-dict.txt\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "with open(cath_dict_path, 'r') as file:\n",
    "    cath_dict = json.load(file)\n",
    "\n",
    "if not os.path.exists(base_save_folder):\n",
    "    os.makedirs(base_save_folder)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "selected_columns = [\n",
    "    # \"composition_ALA\",\"composition_CYS\",\"composition_ASP\",\"composition_GLU\",\"composition_PHE\",\"composition_GLY\",\n",
    "    # \"composition_HIS\",\"composition_ILE\",\"composition_LYS\",\"composition_LEU\",\"composition_MET\",\"composition_ASN\",\n",
    "    # \"composition_PRO\",\"composition_GLN\",\"composition_ARG\",\"composition_SER\",\"composition_THR\",\"composition_VAL\",\n",
    "    # \"composition_TRP\",\"composition_UNK\",\"composition_TYR\",\n",
    "    # \"ss_prop_alpha_helix\",\"ss_prop_beta_bridge\",\"ss_prop_beta_strand\",\n",
    "    # \"ss_prop_3_10_helix\",\"ss_prop_pi_helix\",\n",
    "    # \"ss_prop_hbonded_turn\",\"ss_prop_bend\",\"ss_prop_loop\",\n",
    "    \"hydrophobic_fitness\",\n",
    "    \"isoelectric_point\",\n",
    "    \"charge\",\n",
    "    \"mass\",\n",
    "    \"num_residues\",\n",
    "    \"packing_density\",\n",
    "    \"budeff_total\",\n",
    "    \"evoef2_ref_total\",\n",
    "    \"dfire2_total\",\n",
    "    \"rosetta_total\",\n",
    "    \"aggrescan3d_avg_value\"\n",
    "]\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "def filter_for_archetypes(df, cath_dict):\n",
    "    archetype_ids = []\n",
    "    for _, row in df.iterrows():\n",
    "        class_num = str(row['Class number'])\n",
    "        arch_num = str(row['Architecture number'])\n",
    "        top_num = str(row['Topology number'])\n",
    "        try:\n",
    "            protein_id = cath_dict[class_num][arch_num][top_num]['protein_id']\n",
    "            if protein_id[:4] in row['design_name']:\n",
    "                archetype_ids.append(row['design_name'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return df[df['design_name'].isin(archetype_ids)]\n",
    "\n",
    "df_filtered = filter_for_archetypes(df, cath_dict)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "df_selected_cleaned = df_filtered[selected_columns].dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_selected_cleaned)\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "scaled_features_robust = robust_scaler.fit_transform(df_selected_cleaned)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_features_robust)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "pca_df['architecture_name'] = df_filtered['architecture_name'].reset_index(drop=True)\n",
    "\n",
    "print(\"Variance explained by each component:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Plotting PCA scatter with KDE\n",
    "plt.figure(figsize=(30, 20))\n",
    "unique_architectures = pca_df['architecture_name'].unique()\n",
    "palette = sns.color_palette(\"tab20\", n_colors=len(unique_architectures))\n",
    "architecture_color_mapping = {arch: color for arch, color in zip(unique_architectures, palette)}\n",
    "\n",
    "# KDE overlay for each architecture\n",
    "for architecture, group_df in pca_df.groupby('architecture_name'):\n",
    "    # Printing the number of data points per architecture\n",
    "    print(f\"Number of data points for architecture '{architecture}': {len(group_df)}\")\n",
    "    \n",
    "    sns.kdeplot(\n",
    "        x=group_df['PC1'], y=group_df['PC2'], \n",
    "        color=architecture_color_mapping[architecture],\n",
    "        levels=2, fill=True, alpha=0.2\n",
    "    )\n",
    "\n",
    "# Scatter plot for visualization\n",
    "sns.scatterplot(\n",
    "    x='PC1', y='PC2',\n",
    "    hue='architecture_name', palette=architecture_color_mapping,\n",
    "    data=pca_df, legend='full', s=100\n",
    ")\n",
    "plt.title('PCA on Dataset by Protein Architecture')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Architecture', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "scatter_plot_path = os.path.join(base_save_folder, 'pca_scatter_plot_by_architecture_with_kde.png')\n",
    "plt.savefig(scatter_plot_path, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca complete for: sandwich (2,60).\n"
     ]
    }
   ],
   "source": [
    "#ALL TOPOLOGIES BY ARCHITECTURES WITH JUST ARCHETYPES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Load the dataset\n",
    "path = \"/Volumes/dax-hd/project-data/search-files/merged-data.csv\"\n",
    "base_save_folder = \"/Volumes/dax-hd/project-data/images/pca_archetypal_\"\n",
    "cath_dict_path = \"/Volumes/dax-hd/project-data/search-files/cath-archetype-dict.txt\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "with open(cath_dict_path, 'r') as file:\n",
    "    cath_dict = json.load(file)\n",
    "if not os.path.exists(base_save_folder):\n",
    "    os.makedirs(base_save_folder)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Add the architecture name to df\n",
    "def add_topology_description(df, cath_dict):\n",
    "    def get_topology_description(row):\n",
    "        class_num = str(row['Class number'])\n",
    "        arch_num = str(row['Architecture number'])\n",
    "        top_num = str(row['Topology number'])\n",
    "        try:\n",
    "            description = cath_dict[class_num][arch_num][top_num]['description']\n",
    "            return description\n",
    "        except KeyError:\n",
    "            return \"Unknown\"\n",
    "    \n",
    "    df['topology_description'] = df.apply(get_topology_description, axis=1)\n",
    "    return df\n",
    "\n",
    "df = add_topology_description(df, cath_dict)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Filtering DF for archetypal topologies\n",
    "def filter_for_archetypes(df_filtered, cath_dict):\n",
    "    archetype_ids = []\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        class_num = str(row['Class number'])\n",
    "        arch_num = str(row['Architecture number'])\n",
    "        top_num = str(row['Topology number'])\n",
    "        try:\n",
    "            protein_id = cath_dict[class_num][arch_num][top_num]['protein_id']\n",
    "            if protein_id[:4] in row['design_name']:\n",
    "                archetype_ids.append(row['design_name'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return df_filtered[df_filtered['design_name'].isin(archetype_ids)]\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "selected_architectures = [\"sandwich (2,60)\"]\n",
    "\n",
    "selected_columns = [\n",
    "    # \"composition_ALA\",\"composition_CYS\",\"composition_ASP\",\"composition_GLU\",\"composition_PHE\",\"composition_GLY\",\n",
    "    # \"composition_HIS\",\"composition_ILE\",\"composition_LYS\",\"composition_LEU\",\"composition_MET\",\"composition_ASN\",\n",
    "    # \"composition_PRO\",\"composition_GLN\",\"composition_ARG\",\"composition_SER\",\"composition_THR\",\"composition_VAL\",\n",
    "    # \"composition_TRP\",\"composition_UNK\",\"composition_TYR\",\n",
    "    # \"ss_prop_alpha_helix\",\"ss_prop_beta_bridge\",\"ss_prop_beta_strand\",\"ss_prop_3_10_helix\",\"ss_prop_pi_helix\",\n",
    "    # \"ss_prop_hbonded_turn\",\"ss_prop_bend\",\"ss_prop_loop\",\n",
    "    \"hydrophobic_fitness\",\n",
    "    \"isoelectric_point\",\n",
    "    \"charge\",\n",
    "    \"mass\",\n",
    "    \"num_residues\",\n",
    "    \"packing_density\",\n",
    "    \"budeff_total\",\n",
    "    \"evoef2_ref_total\",\n",
    "    \"dfire2_total\",\n",
    "    \"rosetta_total\",\n",
    "    \"aggrescan3d_avg_value\"\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "for architecture_name in df['architecture_name'].unique():\n",
    "    if architecture_name not in selected_architectures:\n",
    "        continue\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_filtered = df[df['architecture_name'] == architecture_name]\n",
    "    df_filtered_archetype = filter_for_archetypes(df_filtered, cath_dict)\n",
    "    df_selected_cleaned = df_filtered_archetype[selected_columns].dropna()\n",
    "\n",
    "    if df_selected_cleaned.empty or len(df_selected_cleaned) < 3:\n",
    "        print(f\"Skipping {architecture_name}: too few structures available.\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"pca complete for: {architecture_name}.\")\n",
    "\n",
    "    save_folder = os.path.join(base_save_folder, architecture_name.replace('/', '_'))\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------\n",
    "    # Standardize and PCA\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df_selected_cleaned)\n",
    "    pca = PCA(n_components=0.95)\n",
    "    principal_components = pca.fit_transform(scaled_features)\n",
    "    if principal_components.shape[1] < 2:\n",
    "        print(f\"Warning: PCA for {architecture_name} resulted in fewer than 2 components.\")\n",
    "        pca_df = pd.DataFrame(data=principal_components, columns=['PC1'])\n",
    "        pca_df['PC2'] = 0\n",
    "    else:\n",
    "        pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, principal_components.shape[1] + 1)])\n",
    "    \n",
    "    pca_df['topology_description'] = df_filtered_archetype['topology_description'][df_selected_cleaned.index].reset_index(drop=True)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------\n",
    "    # Plotting\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    unique_topologies = pca_df['topology_description'].unique()\n",
    "    palette = sns.color_palette('tab20c', n_colors=len(unique_topologies))\n",
    "    topology_color_mapping = {topology: color for topology, color in zip(unique_topologies, palette)}\n",
    "\n",
    "    # KDE overlay for each architecture\n",
    "    for topology, group_df in pca_df.groupby('topology_description'):\n",
    "        sns.kdeplot(x=group_df['PC1'], y=group_df['PC2'], \n",
    "                    color=topology_color_mapping[topology],\n",
    "                    levels=2, fill=True, alpha=0.3)\n",
    "\n",
    "    scatter_plot = sns.scatterplot( x='PC1', y='PC2',\n",
    "                                    hue='topology_description', palette=topology_color_mapping,\n",
    "                                    data=pca_df, s=100)\n",
    "    \n",
    "    plt.title(f'PCA for {architecture_name}')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    handles, labels = scatter_plot.get_legend_handles_labels()\n",
    "    plt.legend([], [], frameon=False)\n",
    "    plt.savefig(os.path.join(save_folder, 'pca_scatter_by_topology.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Save legend separately\n",
    "    fig_legend = plt.figure(figsize=(3, 4))\n",
    "    plt.figlegend(handles, labels, loc='center')\n",
    "    plt.savefig(os.path.join(save_folder, 'legend.png'), bbox_inches='tight')\n",
    "    plt.close(fig_legend)\n",
    "\n",
    "    # Plotting cumulative variance\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.tight_layout()\n",
    "    cumulative_variance_plot_path = os.path.join(save_folder, 'cumulative_explained_variance.png')\n",
    "    plt.savefig(cumulative_variance_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Component loading plot\n",
    "    for i in range(pca.n_components_):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        component_loadings = pca.components_[i]\n",
    "        indices = np.argsort(abs(component_loadings))[::-1]\n",
    "        feature_names = np.array(selected_columns)[indices]\n",
    "        plt.bar(range(len(indices)), component_loadings[indices])\n",
    "        plt.xticks(range(len(indices)), feature_names, rotation=90)\n",
    "        plt.title(f'PCA Component {i+1} Loadings')\n",
    "        plt.ylabel('Loading Value')\n",
    "        feature_contribution_path = os.path.join(save_folder, f'pca_component_{i+1}_loadings.png')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(feature_contribution_path)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each component: [0.53527937 0.24631892 0.21711078]\n"
     ]
    }
   ],
   "source": [
    "#3D TOPOLOGIES BY ARCHITECTURES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Load the dataset\n",
    "path = \"/Volumes/dax-hd/project-data/search-files/merged-data.csv\"\n",
    "base_save_folder = \"/Volumes/dax-hd/project-data/images/3d_pca_topology\"\n",
    "cath_dict_path = \"/Volumes/dax-hd/project-data/search-files/cath-archetype-dict.txt\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "with open(cath_dict_path, 'r') as file:\n",
    "    cath_dict = json.load(file)\n",
    "\n",
    "# Add the architecture name to df\n",
    "def add_topology_description(df, cath_dict):\n",
    "    def get_topology_description(row):\n",
    "        class_num, arch_num, top_num = str(row['Class number']), str(row['Architecture number']), str(row['Topology number'])\n",
    "        return cath_dict.get(class_num, {}).get(arch_num, {}).get(top_num, {}).get('description', \"Unknown\")\n",
    "    \n",
    "    df['topology_description'] = df.apply(get_topology_description, axis=1)\n",
    "    return df\n",
    "\n",
    "df = add_topology_description(df, cath_dict)\n",
    "\n",
    "# Ensure the base_save_folder exists\n",
    "if not os.path.exists(base_save_folder):\n",
    "    os.makedirs(base_save_folder)\n",
    "\n",
    "# Define the architectures to focus on and columns to be used\n",
    "selected_architectures = [\"sandwich (2,60)\"]\n",
    "selected_columns = [\n",
    "    \n",
    "    # \"composition_ALA\",\n",
    "    # \"composition_CYS\",\"composition_ASP\",\n",
    "    # \"composition_GLU\",\n",
    "    # \"composition_PHE\",\"composition_GLY\",\n",
    "    # \"composition_HIS\",\"composition_ILE\",\"composition_LYS\",\n",
    "    # \"composition_LEU\",\n",
    "    # \"composition_MET\",\"composition_ASN\",\n",
    "    # \"composition_PRO\",\"composition_GLN\",\n",
    "    # \"composition_ARG\",\"composition_SER\",\"composition_THR\",\n",
    "    # \"composition_VAL\",\n",
    "    # \"composition_TRP\",\"composition_UNK\",\"composition_TYR\",\n",
    "    # \"ss_prop_alpha_helix\",\"ss_prop_beta_bridge\",\"ss_prop_beta_strand\",\"ss_prop_3_10_helix\",\"ss_prop_pi_helix\",\n",
    "    # \"ss_prop_hbonded_turn\",\"ss_prop_bend\",\"ss_prop_loop\",\n",
    "    # \"hydrophobic_fitness\",\n",
    "    \"isoelectric_point\",\n",
    "    # \"charge\",\n",
    "    \"mass\",\n",
    "    # \"num_residues\",\n",
    "    # \"packing_density\",\n",
    "    \"budeff_total\",\n",
    "    # \"evoef2_ref_total\",\n",
    "    # \"dfire2_total\",\n",
    "    # \"rosetta_total\",\n",
    "    \"aggrescan3d_avg_value\"\n",
    "]\n",
    "\n",
    "# Filter the DataFrame for the specific architectures\n",
    "def filter_for_archetypes(df, cath_dict):\n",
    "    archetype_ids = [row['design_name'] for _, row in df.iterrows() if cath_dict.get(str(row['Class number']), {}).get(str(row['Architecture number']), {}).get(str(row['Topology number']), {}).get('protein_id', '')[:4] in row['design_name']]\n",
    "    return df[df['design_name'].isin(archetype_ids)]\n",
    "\n",
    "df_filtered = filter_for_archetypes(df, cath_dict)\n",
    "\n",
    "# Perform PCA and plot for each architecture\n",
    "for architecture_name in df['architecture_name'].unique():\n",
    "    if architecture_name not in selected_architectures:\n",
    "        continue\n",
    "    \n",
    "    df_architecture = df[df['architecture_name'] == architecture_name]\n",
    "    df_selected_cleaned = df_architecture[selected_columns].dropna()\n",
    "    \n",
    "    if df_selected_cleaned.empty:\n",
    "        print(f\"No data available for {architecture_name}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df_selected_cleaned)\n",
    "    \n",
    "    # Perform PCA for 3 components\n",
    "    pca = PCA(n_components=3)\n",
    "    principal_components = pca.fit_transform(scaled_features)\n",
    "    pca_df = pd.DataFrame(principal_components, columns=['PC1', 'PC2', 'PC3'])\n",
    "    pca_df['topology_description'] = df_architecture['topology_description'].loc[df_selected_cleaned.index].reset_index(drop=True)\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(30, 21))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'], c=pd.factorize(pca_df['topology_description'])[0], cmap='tab20c', s=20)\n",
    "\n",
    "    ax.set_title(f'3D PCA for {architecture_name}')\n",
    "    ax.set_xlabel('Principal Component 1')\n",
    "    ax.set_ylabel('Principal Component 2')\n",
    "    ax.set_zlabel('Principal Component 3')\n",
    "    \n",
    "    # Legend with unique labels\n",
    "    unique_labels = pd.factorize(pca_df['topology_description'])[1]\n",
    "    custom_legend = [plt.Line2D([0], [0], marker='o', linestyle='None', markersize=10, label=unique_labels[i], color=scatter.cmap(scatter.norm(i))) for i in range(len(unique_labels))]\n",
    "    ax.legend(handles=custom_legend, bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Topologies\")\n",
    "    \n",
    "    plt.savefig(os.path.join(base_save_folder, f'{architecture_name.replace(\"/\", \"_\")}_3d_pca.png'))\n",
    "    plt.close()\n",
    "\n",
    "print(\"Variance explained by each component:\", (pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modules",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
